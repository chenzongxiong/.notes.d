#+TITLE: Play with Kunbernetes
#+AUTHOR: czx
#+EMAIL: zongxiongchen@163.com
#+TODO: "should deep in scheduler of kubernetes"
#+BIND: org-export-publishing-directory "./html"
#+OPTIONS: email:t author:t \n:t ^:nil creator:nil toc:t todo:t *:t
#+INFOJS_OPT: view:showall
#+STARUP: overview
* Play with kunbernetes
  #+BEGIN_SRC sh :exports both :eval query
  uname -r
  #+END_SRC
** Setting up Kubernetes
*** Setting up Kubernetes Master
**** Install etcd and Kubernetes.
#+BEGIN_SRC sh :export both :eval never
  yum install -y etcd kubernetes
#+END_SRC
**** Configure etcd inside "/etc/etcd/etcd.conf"
#+BEGIN_EXAMPLE
  ETCD_NAME=deafult
  ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
  ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
  ETCD_ETCD_ADVERTISE_CLIENT_URLS="http://localhost:2379"
#+END_EXAMPLE
**** Configure kubernets api server inside "/etc/kubernetes/apiserver"
#+BEGIN_EXAMPLE
  KUBE_API_ADDRESS="--address=0.0.0.0"
  KUBE_API_PORT="--port=8080"
  KUBELET_PORT="--kubelet_port=10250"
  KUBE_ETCD_SERVERS="--etcd_servers=http://127.0.0.1:2379"
  KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"
  KUBE_ADMISSION_CONTROL="--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota"
  KUBE_API_ARGS=""
#+END_EXAMPLE
**** Start and enable service "etcd, kube-apiserver, kube-controller-manager and kuber-scheduler"
**** Define flannel network configuration in etcd
#+BEGIN_SRC sh :eval never
  etcdctl mk /atomic.io/network/config '{"Network": "172.17.0.0/16"}'
#+END_SRC
**** Get node info
#+BEGIN_SRC sh :export both :eval never
  kubectl version
  kubectl get nodes
#+END_SRC
*** Setting up Kubernetes Minion
**** Install Kubernetes and flannel
#+BEGIN_SRC sh :eval never
  yum install -y flannel kubernetes
#+END_SRC
**** Configure flannel service inside "/etc/sysconfig/flanneld"
    #+BEGIN_EXAMPLE
    FLANNEL_ETCD="http://10.18.5.79:2379"
    #+END_EXAMPLE
**** Configure kubernetes inside "/etc/kubernetes/config"
#+BEGIN_EXAMPLE
  KUBE_MASTER="--master=http://10.18.5.79:8080"
#+END_EXAMPLE
**** Configure kubelet service inside "/etc/kubernetes/kubelet"
#+BEGIN_EXAMPLE
  KUBELET_ADDRESS="--address=0.0.0.0"
  KUBELET_PORT="--port=10250"
  # change the hostname to this hostâ€™s IP address
  KUBELET_HOSTNAME="--hostname_override=10.18.5.79"
  KUBELET_API_SERVER="--api_servers=http://10.18.5.79:8080"
  KUBELET_ARGS=""
#+END_EXAMPLE
**** Start and enable service "kube-proxy, kubelet, docker, flanneld"
**** Check the interface of "flannel0"
#+BEGIN_SRC sh :exports both :eval never
  ip a | grep flannel | grep inet
#+END_SRC
**** Check the node on master
    #+BEGIN_SRC sh :exports both :eval never
  kubectl get nodes
    #+END_SRC
**** Create Pod
**** Create file name "mysql.yaml"
    #+BEGIN_EXAMPLE
    #+INCLUDE: "/root/pods/mysql.yaml"
    #+END_EXAMPLE
**** Create the Pod
#+BEGIN_SRC sh :eval never
  kubectl create -f /root/pods/mysql.yaml
#+END_SRC
**** Get the pods
#+BEGIN_SRC sh :exports both :eval never
  kubectl get pods
#+END_SRC
**** Create Service
**** Create file named "mysql-service.yaml"
#+BEGIN_EXAMPLE
#+INCLUDE: "/root/pods/mysql-service.yaml"
#+END_EXAMPLE
**** Create the Service
#+BEGIN_SRC sh :eval never
  kubectl create -f /root/pods/mysql-service.yaml
#+END_SRC
**** Get the Service
#+BEGIN_SRC sh :export both :eval never
  kubectl get services
#+END_SRC
**** Problems I encounter while installing the kubernetnes clusters
**** Solving the confiction with `docker` and `docker-engine-1.8.3` package
#+BEGIN_SRC sh :eval never
  yum remove docker-engine                # remove the docker-engine package to avoid confict
  yum install -y kubernetes-node          # this package depend on `docker`
  rpm -e --nodeps docker                  # remove the `docker` without dependencies checking
  yum install -y docker-engine            # install docker-engine
#+END_SRC
*** Why pull from grc.io registry when I create pod?
**** Debug log:
    - Sun, 21 Feb 2016 00:18:31 +0800       Sun, 21 Feb 2016 00:45:38 +0800 26      {kubelet 10.18.5.79}    implicitly required container POD       failed          Failed to pull image "gcr.io/google_containers/pause:0.8.0": image pull failed for gcr.io/google_containers/pause:0.8.0, this may be because there are no credentials on this request.  details: (API error (500): unable to ping registry endpoint https://gcr.io/v0/
    - v2 ping attempt failed with error: Get https://gcr.io/v2/: dial tcp 74.125.203.82:443: i/o timeout
    - v1 ping attempt failed with error: Get https://gcr.io/v1/_ping: dial tcp 74.125.203.82:443: i/o timeout)
**** Solution 1
#+BEGIN_EXAMPLE
  kubelet --pod-infra-container-image="docker.io/kubernetes/pause"
  #+END_EXAMPLE
or
#+BEGIN_EXAMPLE
  docker pull kubernetes/pause
  docker tag kubernetes/pause gcr.io/google_containers/pause:0.8.0
#+END_EXAMPLE
**** Solution 2
     add the /etc/default/docker on ubuntu or /etc/sysconfig/docker on centos
     #+BEGIN_EXAMPLE
            export http_proxy=http://hostname:port
            export https_proxy=http://hostname:port
     #+END_EXAMPLE
     then restart the docker daemon service
**** Ref
     - [[https://github.com/kubernetes/kubernetes/issues/7090][Why pull from "gcr.io" registry when I create pod?]]
     - [[https://github.com/kubernetes/kubernetes/issues/6888][No such image: gcr.io/google_containers/redis]]
** Kubernetes architecture
*** Architecture
**** #+ATTR_HTML: :alt kubernetes/architecture.png images :title architecture :align center
     [[../images/kubernetes/architecture.png]]
**** Self healing
     - auto restarting
     - re-scheduling
     - replicating container require active controllers, not just imperative orchestration
*** Basic Concept
**** Pods: groups of containers, share the resource(volumes, cpus, rams, etc.)
      + network namespace (applications within the pod have access to the same IP and port space)
      + PID namespace (applications within the pod can see each other's processes)
      + IPC namespace (applications within the pod can use SystemV IPC or POSIX message queues to communicate)
      + UTS namespace (applications within the pod share a hostname)
     #+BEGIN_EXAMPLE
     pod nerver reschedule to a new nodes, they must be replace(replication controller)
     #+END_EXAMPLE
     - Phase
       + Pending
       + Running
       + Succeeded
       + Failed
       + Unknown
     - Status
       + True: container pass
       + False: container failed
       + Unknown: not receive response from apiserver
     - RestartPolicy
       + Alway
       + OnFailure
       + Never
     - Why not just run multiple programs in a single(Docker) container?
       + Transparency.
       + Decoupling software dependencies
       + Ease of use
       + Efficiency
**** Label && Selectors
     #+BEGIN_EXAMPLE
     key/value pairs that attached to objects, such as pod.
     Labels are intended to be used to specify identifying attributes of objects that are meaningful
     and relevant to users, but which don't directly imply semantics to the core system.
     do not provide uniqueness.
     #+END_EXAMPLE
**** Replication Controller
     #+BEGIN_EXAMPLE
     ensures that a specified number of pod "replicas"are running at any one time.
     only appropriate for pods with RestartPolicy = Always
     #+END_EXAMPLE
     - once pod is created, there is no relationship between pods and templates(define in yaml or json)
     - no scheduling policies in replication controller
     - it itself not perform readiness nor liveness probes
     - currently, only terminated pods are excluded from its count. (PENDING, RUNNING are included)
***** re-scheduling
***** scaling
***** rolling updates
      updates to a service by replacing pods one-by-one
**** Services
     pods are mortal. they are born and they die. so the IP not stable over time.
     an abstraction which defines a logical set of Pods and a policy by which to access them
     #+BEGIN_EXAMPLE
     solve the comminucation between two pods.
     decompling the fronted(pod 1) and backend(pod 2)
     #+END_EXAMPLE
     the service will be assigned an IP address (cluster IP), tied to the lifespan of the Service.
     service can be used without selectors
     services are a "layer 3"(TCP or UDP over ip) not "layer 7" (HTTP)

**** Namespaces
     supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces.
**** Volumes(liftime along with pod, not container)
**** Annotations
**** Names(create by client), Identifiers, UIDs(generate by kubernetes)
**** Jobs(not supported in v1)
     a job creates one or more pods and ensures that a specified number of them successfully terminate.
*** Kube ctroller node
    - make global decisions about the cluster(eg., scheduling)
    - detecting and responding to cluster events
    (eg., starting up a new pod when a replication controller's replicas field is unsatisfied)
**** etcd
     Distrubed Watchable Storage
     Used as Kubernetes' backing store. All cluster data is stored here
**** api server
**** kube-controller-manager
     background threads that handle routine tasks in the cluster.
     - node controller
       - responsible for noticing & responding when nodes go down
     - replication controller
       - responsible for maintaining the correct number of pods for every replication controler object in the system
     - endpoints controller
       - populates the endpoints object(i.e., join services & pods)
     - service account & token controllers
       - create default account and API access tokens for new namespaces.
     - some others
**** kube scheduler
     The scheduler is pluggable. unscheduled pods to nodes via the /binding API
***** How Pods with Resource Requests are scheduled
      #+BEGIN_EXAMPLE
              When a pod is created, the Kubernetes scheduler selects a node for the pod to run on.
      Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for pods.
      The scheduler ensures that, for each resource type (CPU and memory), the sum of the resource requests of the containers
      scheduled to the node is less than the capacity of the node. Note that although actual memory or CPU resource usage on
      nodes is very low, the scheduler will still refuse to place pods onto nodes if the capacity check fails. This protects
      against a resource shortage on a node when resource usage later increases, such as due to a daily peak in request rate.
      #+END_EXAMPLE
      If a container exceeds its memory limit, it may be terminated.
      A container may or may not be allowed to exceed its CPU limit for extended periods of time. However, it will not be killed for excessive CPU usage.
**** addons
     - DNS: provides cluster local DNS.
     - kube-ui
     - fluentd-elasticsearch: provides log storage.
     - cluster-monitoring: providing monitoring for the cluster.
**** Kube controller manager server
     The replicationcontroller is a mechanism that is layered on top of the simple pod API.
     We eventually plan to port it to a generic plug-in mechanism, once one is implemented.
**** Accessing the API from a Pod
     By kube-system, a pod is associated with a service account, and a credential(token) for that service account is placed into
     filesystem tree of each container in pod, at /var/run/secrets/kubernetes.io/serviceaccount/token.
**** Accessing services running on the cluster
     - access services through public IPs
       - use a service with type 'NodePort' or 'LoadBalancer'. ref: kubectl expose
     - access services, nodes, or pods using the Proxy Verb
     - access from a node or pod in the cluster
**** Proxies
     - kube proxy
       - proxies UDP and TCP
       - not understand HTTP
       - load balancing
       - is just used to reach services
     - apiserver proxy
       - connects a user outside of the cluster to cluster IPs which otherwise might not be reachable
       - client to proxy uses HTTPS(or http if configured)
       - can e used to reach sa Node, Pod or Service
       - does load balancing when used to reach a Service
     - kubectl proxy
       - run in a pod
       - client to proxy uses HTTP
       - proxy to apiserver uses HTTPS
       - locates apiserver
       - adds authentication headers

*** Kube slave Node
    - node status
    - node address
      - hostname
      - external ip
      - internal ip
    - node phase
      - pending
      - running
        - Running phase is neccessary but not sufficent requirement for scheduling Pods.
      - terminated
    - node condition
    - node capacity
      - CPUs
      - memory
      - maximum number of pods that can be scheduld onto the node
    - node info, gather by 'kubelet' from the node
      - kubernets version (kublete version, kube-proxy version)
      - docker version
      - OS name
**** kubelet
    The kubelet manages pods and their containers, their images, their volumes, etc.
**** kube-proxy
    - a simple network proxy
    - load blancer
**** Network in Kubernetes
    - differently than Docker does by deafult.
      + Highly-coupled container-to-container communications: this is solved by pods and localhost communications
      + Pod-to-Pod communications
      + Pod-to-Service communications
      + External-to-Service communications
    - impose the following fundamental requirements
    #+BEGIN_EXAMPLE
      Dynamic port allocation brings a lot of complications to the system - every application has to take ports as flags,
      the API servers have to know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc.
    #+END_EXAMPLE
      + all containers can communicate with all other containers without NAT
      + all nodes can communicate with all containers (and vice-versa) without NAT
      + the IP that a container sees itself as is the same IP that other see it as
    #+BEGIN_EXAMPLE
      for Kubernetes to enable low-friction porting of apps from VMs to containers.
    #+END_EXAMPLE
      + Pods more like VM or physical host from the perspectives of port allocation
      +

    - [[http://kubernetes.io/v1.1/docs/design/networking.html][how to achives this?]]
        + OpenVSwitch
          - gre
          - VxLAN
        + Flannel
        + Weave
        + Calico
      + ref:
        + http://kubernetes.io/v1.1/docs/admin/networking.html
        + http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/
** Compare Kunbernetes with Mesos
| Name       | Scheduler | Scalability |   |   |
|------------+-----------+-------------+---+---|
| Kubernetes |           |             |   |   |

** Analysis Tools
*** Kubemark
    - [[https://github.com/kubernetes/kubernetes/blob/2b2f2857771c748f0f0e261f3b8e2ad1627325ce/docs/proposals/kubemark.md][kubemark]]
*** Plot
    - [[https://github.com/coreos/kscale/blob/dfe65f050cff5bebf83074e3b3e6b3c2d69a9222/logplot/main.go][plot-tool]]
*** Metrics
    Kubernetes components provide metrics for most end-to-end API calls.
    - [[https://prometheus.io/][Prometheus]]
*** remove the rate limiter `CreatePod`
    - [[https://github.com/kubernetes/kubernetes/pull/17885][ client request metrics should be registered only once #17885 ]]
*** problem of scheduler
    - [[https://github.com/kubernetes/kubernetes/issues/18091][Scheduler Performance Testing Proposal #18091]]
    - [[https://docs.google.com/presentation/d/1HYGDFTWyKjJveAk_t10L6uxoZOWTiRVLLCZj5Zxw5ok/edit?pref=2&pli=1#slide=id.p][Scheduler Performance Test]]
    - [[https://github.com/kubernetes/kubernetes/pull/18458][scheduler performance test pull request #17885]]
    - [[https://github.com/kubernetes/kubernetes/issues/18126][scheduler: using inefficient math library causes significant slowdown #18126]]
    - [[https://github.com/kubernetes/kubernetes/pull/18458][Scheduler Performance Testing #18458]]
    - [[https://github.com/kubernetes/kubernetes/issues/18418][Tracking issue for issues related to scheduler performance/scalability #18418]]
    - [[https://github.com/kubernetes/kubernetes/wiki/SIG-Scalability][Scalability SIG]]

*** New profiling result for schedule 1000 pods over 1000 nodes
    - [[https://storage.googleapis.com/profiling/fix.svg][fix]]
    - [[https://storage.googleapis.com/profiling/org.svg][org]]
*** Kunbernetes can support hundreds of machines, the open source version of `Borg
*** refs:
    - [[https://news.ycombinator.com/item?id=9653865][kubernetes makes for an amazing an developer story]]
** Useful command
*** kubectl
#+BEGIN_SRC sh :export code :eval never
  kubectl config view             # check the location and credentials that kubectl knows about.
  kubectl get nodes
  kubectl get services
  kubectl get pods
  kubectl get rc or kubectl get replicationcontrollers
  kubectl get pod <pod-name>
  kubectl get service <service-name>
  kubectl get rc <replcation-controller-name>
  kubectl describe pod <pod-name>
  kubectl describe service <service-name>
  kubectl get pods -l 'filtering' # equality-bsaed requirement  or set-base requirement
  kubectl delete rc my-nginx      # rc: replication controller
  kubectl delete svc my-nginx     # svc: service
  kubectl logs <pod-name>
  kubectl get ep or kubectl get endpoints
#+END_SRC

#+NAME: Launching a simpe application
#+BEGIN_EXAMPLE
  kubectl run my-nginx --image=nginx --replicas=2 --port=80
  kubectl get pods
#+END_EXAMPLE
#+NAME: Exposing your application to the Internet
#+BEGIN_EXAMPLE
  kubectl expose rc my-nginx --port=80 --type=LoadBalancer
#+END_EXAMPLE
*** etcdctl
#+BEGIN_SRC sh
  etcdctl ls <dir>
  etcdctl get <key>
#+END_SRC
